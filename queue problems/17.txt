LRU Cache Implementation

Difficulty Level : Hard
Read
Discuss(110+)
Courses
Practice
Video
How to implement LRU caching scheme? What data structures should be used? 
We are given the total possible page numbers that can be referred to. We are also given a cache (or memory) size (The number of page frames that the cache can hold at a time). The LRU caching scheme is to remove the least recently used frame when the cache is full and a new page is referenced which is not there in the cache. Please see the Galvin book for more details (see the LRU page replacement slide here)

Recommended: Please solve it on “PRACTICE” first, before moving on to the solution.
LRU cache implementation using queue and hashing:
To solve the problem follow the below idea:

We use two data structures to implement an LRU Cache.  

Queue is implemented using a doubly-linked list. The maximum size of the queue will be equal to the total number of frames available (cache size). The most recently used pages will be near the front end and the least recently used pages will be near the rear end.
A Hash with the page number as key and the address of the corresponding queue node as value.
When a page is referenced, the required page may be in the memory. If it is in the memory, we need to detach the node of the list and bring it to the front of the queue. 
If the required page is not in memory, we bring that in memory. In simple words, we add a new node to the front of the queue and update the corresponding node address in the hash. If the queue is full, i.e. all the frames are full, we remove a node from the rear of the queue, and add the new node to the front of the queue.


Example – Consider the following reference string:  1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5

Find the number of page faults using the least recently used (LRU) page replacement algorithm with 3-page frames. 

Below is the illustration of the above approach:
 



 



Note: Initially no page is in the memory.

 
CPP-STL-Self-Paced-Course

Follow the below steps to solve the problem:


Create a class LRUCache with declare a list of type int, an unordered map of type <int, list<int>>, and a variable to store the maximum size of the cache
In the refer function of LRUCache
If this value is not present in the queue then push this value in front of the queue and remove the last value if the queue is full
If the value is already present then remove it from the queue and push it in the front of the queue
In the display function print, the LRUCache using the queue starting from the front
Below is the implementation of the above approach:

C++
C
Java
Python3
# We can use stl container list as a double
# ended queue to store the cache keys, with
# the descending time of reference from front
# to back and a set container to check presence
# of a key. But to fetch the address of the key
# in the list using find(), it takes O(N) time.
# This can be optimized by storing a reference
# (iterator) to each key in a hash map.
class LRUCache:
    # store keys of cache
    def __init__(self, n):
        self.csize = n
        self.dq = []
        self.ma = {}
 
 
    # Refers key x with in the LRU cache
    def refer(self, x):
         
        #  not present in cache
        if x not in self.ma.keys():
            # cache is full
            if len(self.dq) == self.csize:
                # delete least recently used element
                last = self.dq[-1]
 
                # Pops the last element
                ele = self.dq.pop();
 
                # Erase the last
                del self.ma[last]
 
        # present in cache
        else:
            del self.dq[self.ma[x]]
 
        # update reference
        self.dq.insert(0, x)
        self.ma[x] = 0;
 
    # Function to display contents of cache
    def display(self):
 
        # Iterate in the deque and print
        # all the elements in it
        print(self.dq)
 
# Driver Code
ca = LRUCache(4)
 
ca.refer(1)
ca.refer(2)
ca.refer(3)
ca.refer(1)
ca.refer(4)
ca.refer(5)
ca.display()
# This code is contributed by Satish Srinivas
C#
Javascript
Output
5 4 1 3 
Time Complexity: The time complexity of the refer() function is O(1) as it does a constant amount of work.
Auxiliary Space: The space complexity of the LRU cache is O(n), where n is the maximum size of the cache.

Java Implementation using LinkedHashMap. 
Approach: The idea is to use a LinkedHashSet that maintains the insertion order of elements. This way implementation becomes short and easy.

Below is the implementation of the above approach:

C++
#include <iostream>
#include <list>
#include <unordered_map>
using namespace std;
 
class LRUCache {
 
private:
    int capacity;
    list<int> cache;
    unordered_map<int, list<int>::iterator> map;
 
public:
    LRUCache(int capacity) : capacity(capacity) { }
 
      // This function returns false if key is not
    // present in cache. Else it moves the key to
    // front by first removing it and then adding
    // it, and returns true.
    bool get(int key) {
        auto it = map.find(key);
        if (it == map.end()) {
            return false;
        }
        cache.splice(cache.end(), cache, it->second);
        return true;
    }
 
    void refer(int key) {
        if (get(key)) {
            return;
        }
        put(key);
    }
 
    // displays contents of cache in Reverse Order
    void display() {
        for (auto it = cache.rbegin(); it != cache.rend(); ++it) {
           
         // The descendingIterator() method of
        // java.util.LinkedList class is used to return an
        // iterator over the elements in this LinkedList in
        // reverse sequential order
            cout << *it << " ";
        }
    }
 
    void put(int key) {
        if (cache.size() == capacity) {
            int first_key = cache.front();
            cache.pop_front();
            map.erase(first_key);
        }
        cache.push_back(key);
        map[key] = --cache.end();
    }
};
 
int main() {
    LRUCache cache(4);
    cache.refer(1);
    cache.refer(2);
    cache.refer(3);
    cache.refer(1);
    cache.refer(4);
    cache.refer(5);
    cache.display();
    return 0;
}
 
// This code is contributed by divyansh2212
Java
Output
5 4 1 3 
Time Complexity: O(1), we use a Linked HashSet data structure to implement the cache. The Linked HashSet provides constant time complexity for both adding elements and retrieving elements.
Auxiliary Space: O(n), we need to store n elements in the cache, so the space complexity is O(n).

Related Article:
Python implementation using OrderedDict
 
This article is compiled by Aashish Barnwal and reviewed by the GeeksforGeeks team. Please write comments if you find anything incorrect, or if you want to share more information about the topic discussed above.
 



Like
Previous
Least Frequently Used (LFU) Cache Implementation
Next
Implement Stack using Queues
Related Articles
1.
LRU Cache implementation using Double Linked Lists
2.
Implementation of Least Recently Used (LRU) page replacement algorithm using Counters
3.
Page Faults in LRU | Implementation
4.
Locality of Reference and Cache Operation in Cache Memory
5.
Least Frequently Used (LFU) Cache Implementation
6.
LRU Approximation (Second Chance Algorithm)
7.
LRU Full Form
8.
Program for Least Recently Used (LRU) Page Replacement algorithm
9.
How to Implement Forward DNS Look Up Cache?
10.
Cache Memory in Computer Organization
Article Contributed By :
https://media.geeksforgeeks.org/auth/avatar.png
GeeksforGeeks
Vote for difficulty
Current difficulty : Hard
Easy
Normal
Medium
Hard
Expert
Improved By :
_Gaurav_Tiwari
spattk
MadhaviVaddepalli
Majorssn
nirajtechi
sudhanshublaze
adnanirshad158
sumitgumber28
sharanaseem
swetha_vazhakkat
janardansthox
surajrasr7277
factworx4i2
phasing17
divyansh2212
Article Tags :
Amazon
cpp-unordered_map
MakeMyTrip
Morgan Stanley
Snapdeal
STL
Advanced Data Structure
GATE CS
Operating Systems
Queue
Practice Tags :
Amazon
MakeMyTrip
Morgan Stanley
Snapdeal
Advanced Data Structure
Operating Systems
Queue
STL
Report Issue